# Weather-based Temperature Prediction â€“ Stacking Regressor (XGBoost + CatBoost)

## ğŸ“Œ Project Overview
This project focuses on predicting the **next day's average temperature deviation** using historical meteorological data from multiple weather stations.  
The goal is to predict how much warmer or cooler the next day will be compared to the climatological temperature (historical average).  
We engineered advanced weather features and combined multiple regression models using a **Stacking Regressor** to maximize accuracy.

---

## ğŸ¯ Objectives
- Handle multi-year meteorological data with missing values and anomalies  
- Perform advanced feature engineering to capture daily, seasonal, and spatial weather patterns  
- Train base models (XGBoost, CatBoost) and combine them using Ridge Regression as meta-learner  
- Optimize hyperparameters using Optuna to minimize RMSE  
- Generalize model performance on unseen stations (Paju, Suwon)

---

## ğŸ› ï¸ Techniques Used

### **Feature Engineering**
- Temporal features: day of year, week of year, weekday/weekend  
- Cyclical encoding for seasonal periodicity (sin, cos transforms)  
- Night-time interaction features (temperature Ã— humidity, vapor pressure changes)  
- Altitude and geographic coordinate combinations (lat Ã— lon, lat Â± lon)  
- Polynomial feature combinations  

### **Data Processing**
- Missing value handling:  
  - `-9999` â†’ `NaN`  
  - Zero-filling for snow depth, sunshine duration  
  - Linear interpolation for continuous variables  
- Outlier clipping with optimized bounds  
- One-hot encoding for categorical variables  

### **Models**
- XGBoost  
- CatBoost (efficient categorical handling)  
- Ridge Regression (meta-learner for stacking)

### **Hyperparameter Optimization**
- Optuna search for best model parameters and outlier thresholds  

### **Evaluation Metrics**
- RMSE (Root Mean Squared Error)  
- RÂ² Score  

---

## ğŸ“‚ Dataset
- **Training Set:**  
  - Stations: Dongducheon, Seoul, Ganghwa, Incheon, Icheon, Yangpyeong  
  - Time: 2019â€“2024  
- **Test Set:**  
  - Stations: Paju, Suwon  
- **Features:**  
  - 19 weather variables (cloud cover, dew point, humidity, pressure, wind, etc.)  
  - Climatology temperature (historical average for that date)  
- **Target:**  
  - `target = next_day_avg_temp - climatology_temp`

---

## ğŸš§ Challenges & Improvements
- Early models (single XGBoost or CatBoost) underperformed (RÂ² â‰ˆ 0.80)  
- Severe overfitting mitigated by:
  - Residual analysis  
  - Ridge regularization in meta-learner  
  - Feature clipping and careful missing value handling  
- Advanced temporal and interaction features improved generalization  
- Stacking approach boosted performance to **RÂ² = 0.842**, outperforming single models  

---

## ğŸ“Š Results
- **XGBoost Alone:** RÂ² = 0.800  
- **CatBoost Alone:** RÂ² â‰ˆ 0.81  
- **Stacking (XGBoost + CatBoost â†’ Ridge):**  
  - RÂ² = **0.84217**  
  - RMSE significantly reduced  
- Model demonstrated strong generalization to unseen stations (Paju, Suwon)

---

## ğŸ” Additional Result Analysis
- Stacking improved RÂ² by 4â€“5% over single models  
- Advanced features (night-time interactions, altitude-adjusted temps) were top contributors  
- Hyperparameter tuning with Optuna refined outlier clipping for better stability  
- Missing value imputation and temporal encoding reduced bias in cold/humid conditions  

ğŸ’¡ **Key Insight:**  
Feature engineering and stacking synergistically improved model accuracy and generalization.  
Weather prediction benefits from combining multiple learners and rich temporal/geospatial features  
rather than relying solely on a single complex model.

---

## ğŸš€ How to Run
1. Mount datasets (train/test/station info)  
2. Run `preprocessing.py` â†’ handles missing values, creates features  
3. Run `train_models.py` â†’ trains XGBoost, CatBoost, and Ridge Stacking  
4. Run `predict.py` â†’ generates submission CSV for test stations  

---

## ğŸ”‘ Future Improvements
- Incorporate lagged weather variables for time-series context  
- Experiment with neural networks (LSTM, Temporal CNN)  
- Integrate external data sources (satellite, climate indices)  
- Advanced imputation using KNN or multivariate methods  

---

## ğŸ” Leaderboard Ranking & Visuals

### Private Leaderboard Ranking
<img width="953" height="552" alt="image" src="https://github.com/user-attachments/assets/040e7e52-ad6d-421d-bdcf-921dc1922be8" />


### Feature Correlation 
<img width="733" height="550" alt="image" src="https://github.com/user-attachments/assets/d5de0773-8e98-4ac7-b4f8-07a593f9cba4" />

## ğŸ‘¤ Author
Hyunbin Ki (Model design, feature engineering, code development)
